{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae012b01-f962-47d1-92a0-4c7d902a896b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and custom RMSE metric defined.\n"
     ]
    }
   ],
   "source": [
    "# Shell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Custom metric to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "print(\"Libraries imported and custom RMSE metric defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84e1156a-533e-4977-920f-d49ad4bd2416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched sentiment data for RELIANCE.NS\n",
      "Fetched sentiment data for SBIN.NS\n",
      "Fetched sentiment data for HDFCBANK.NS\n",
      "Fetched sentiment data for TCS.NS\n",
      "Stock prices and sentiment data fetched and saved.\n"
     ]
    }
   ],
   "source": [
    "# Shell 2: Fetch stock price data and sentiment data\n",
    "stock_symbols = ['RELIANCE.NS', 'SBIN.NS', 'HDFCBANK.NS', 'TCS.NS']\n",
    "api_key = 'NPWUSQC1723OZ4YW'\n",
    "stock_data = {}\n",
    "sentiment_data = {}\n",
    "\n",
    "# Step 1: Fetch stock price data using Yahoo Finance\n",
    "for symbol in stock_symbols:\n",
    "    stock_data[symbol] = yf.download(symbol, start='2019-01-01', end='2024-01-01')\n",
    "    stock_data[symbol].to_csv(f'{symbol}_prices.csv')  # Optional saving as CSV\n",
    "\n",
    "# Step 2: Fetch sentiment data using Alpha Vantage API\n",
    "for symbol in stock_symbols:\n",
    "    url_sentiment = f'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={symbol}&apikey={api_key}'\n",
    "    response_sentiment = requests.get(url_sentiment)\n",
    "    \n",
    "    if response_sentiment.status_code == 200:\n",
    "        sentiment_data[symbol] = response_sentiment.json()\n",
    "        print(f\"Fetched sentiment data for {symbol}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch sentiment data for {symbol}: {response_sentiment.status_code}\")\n",
    "    \n",
    "    time.sleep(15)  # To handle API rate limits\n",
    "\n",
    "# Optionally save sentiment data\n",
    "for symbol, data in sentiment_data.items():\n",
    "    df_sentiment = pd.DataFrame(data.get('feed', []))  # Assuming sentiment data is under 'feed'\n",
    "    df_sentiment.to_csv(f'{symbol}_sentiment.csv', index=False)\n",
    "\n",
    "print(\"Stock prices and sentiment data fetched and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba7e489b-a0e9-4f25-9938-7fdbbdcc4a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RELIANCE.NS':                    Open         High          Low        Close    Adj Close  \\\n",
       " Date                                                                          \n",
       " 2019-01-01  1028.852905  1030.727295  1015.000732  1024.966919  1000.962952   \n",
       " 2019-01-02  1019.023804  1030.453003  1006.680298  1011.617737   987.926453   \n",
       " 2019-01-03  1012.623474  1019.115234   996.714111   999.137085   975.738098   \n",
       " 2019-01-04  1003.388733  1009.834778   988.485107  1004.531616   981.006287   \n",
       " 2019-01-07  1012.166321  1022.635437  1006.680298  1010.109070   986.453125   \n",
       " ...                 ...          ...          ...          ...          ...   \n",
       " 2023-12-22  2559.600098  2580.899902  2547.649902  2565.050049  2556.373779   \n",
       " 2023-12-26  2568.000000  2591.949951  2562.699951  2578.050049  2569.329834   \n",
       " 2023-12-27  2582.000000  2599.899902  2573.100098  2586.850098  2578.100098   \n",
       " 2023-12-28  2589.800049  2612.000000  2586.850098  2605.550049  2596.736816   \n",
       " 2023-12-29  2611.100098  2614.000000  2579.300049  2584.949951  2576.206299   \n",
       " \n",
       "              Volume       Date  \n",
       " Date                            \n",
       " 2019-01-01  4873335 2019-01-01  \n",
       " 2019-01-02  7814409 2019-01-02  \n",
       " 2019-01-03  8144143 2019-01-03  \n",
       " 2019-01-04  9258272 2019-01-04  \n",
       " 2019-01-07  6030145 2019-01-07  \n",
       " ...             ...        ...  \n",
       " 2023-12-22  8270892 2023-12-22  \n",
       " 2023-12-26  3732832 2023-12-26  \n",
       " 2023-12-27  4602078 2023-12-27  \n",
       " 2023-12-28  6151318 2023-12-28  \n",
       " 2023-12-29  5432292 2023-12-29  \n",
       " \n",
       " [1235 rows x 7 columns],\n",
       " 'SBIN.NS':                   Open        High         Low       Close   Adj Close  \\\n",
       " Date                                                                     \n",
       " 2019-01-01  297.500000  300.700012  293.850006  299.600006  281.996796   \n",
       " 2019-01-02  299.100006  302.500000  293.100006  293.899994  276.631714   \n",
       " 2019-01-03  295.000000  295.549988  290.100006  291.100006  273.996246   \n",
       " 2019-01-04  292.100006  299.000000  291.500000  297.649994  280.161377   \n",
       " 2019-01-07  301.049988  301.500000  295.200012  296.299988  278.890686   \n",
       " ...                ...         ...         ...         ...         ...   \n",
       " 2023-12-22  644.750000  649.400024  635.150024  636.750000  626.248047   \n",
       " 2023-12-26  638.849976  641.299988  635.650024  638.049988  627.526550   \n",
       " 2023-12-27  640.750000  649.450012  639.000000  648.549988  637.853394   \n",
       " 2023-12-28  650.250000  653.299988  646.500000  651.400024  640.656433   \n",
       " 2023-12-29  645.500000  649.599976  639.549988  642.049988  631.460632   \n",
       " \n",
       "               Volume  \n",
       " Date                  \n",
       " 2019-01-01  11837127  \n",
       " 2019-01-02  25559853  \n",
       " 2019-01-03  17548347  \n",
       " 2019-01-04  19514041  \n",
       " 2019-01-07  14579399  \n",
       " ...              ...  \n",
       " 2023-12-22  14998068  \n",
       " 2023-12-26  10153089  \n",
       " 2023-12-27  14417646  \n",
       " 2023-12-28  16982092  \n",
       " 2023-12-29  13221898  \n",
       " \n",
       " [1235 rows x 6 columns],\n",
       " 'HDFCBANK.NS':                    Open         High          Low        Close    Adj Close  \\\n",
       " Date                                                                          \n",
       " 2019-01-01  1063.824951  1075.500000  1052.800049  1074.050049  1022.420776   \n",
       " 2019-01-02  1071.400024  1073.750000  1059.849976  1064.250000  1013.091736   \n",
       " 2019-01-03  1062.099976  1064.125000  1051.500000  1055.900024  1005.143188   \n",
       " 2019-01-04  1057.625000  1064.250000  1055.175049  1058.724976  1007.832275   \n",
       " 2019-01-07  1063.849976  1067.675049  1059.000000  1060.324951  1009.355408   \n",
       " ...                 ...          ...          ...          ...          ...   \n",
       " 2023-12-22  1683.599976  1685.900024  1667.099976  1670.849976  1648.341187   \n",
       " 2023-12-26  1673.250000  1685.949951  1668.550049  1682.449951  1659.784912   \n",
       " 2023-12-27  1681.500000  1706.500000  1678.599976  1703.300049  1680.354126   \n",
       " 2023-12-28  1709.300049  1721.400024  1702.000000  1705.250000  1682.277710   \n",
       " 2023-12-29  1697.000000  1714.900024  1696.000000  1709.250000  1686.223877   \n",
       " \n",
       "               Volume  \n",
       " Date                  \n",
       " 2019-01-01   3186720  \n",
       " 2019-01-02   4067116  \n",
       " 2019-01-03   6385832  \n",
       " 2019-01-04   3643560  \n",
       " 2019-01-07   2693506  \n",
       " ...              ...  \n",
       " 2023-12-22  24289425  \n",
       " 2023-12-26   9022928  \n",
       " 2023-12-27  13504539  \n",
       " 2023-12-28  22038235  \n",
       " 2023-12-29  12505713  \n",
       " \n",
       " [1235 rows x 6 columns],\n",
       " 'TCS.NS':                    Open         High          Low        Close    Adj Close  \\\n",
       " Date                                                                          \n",
       " 2019-01-01  1896.000000  1910.000000  1885.000000  1902.800049  1687.065552   \n",
       " 2019-01-02  1905.000000  1934.449951  1900.000000  1923.300049  1705.241455   \n",
       " 2019-01-03  1919.000000  1944.949951  1893.099976  1899.949951  1684.538696   \n",
       " 2019-01-04  1900.000000  1901.199951  1841.000000  1876.849976  1664.057739   \n",
       " 2019-01-07  1891.800049  1908.800049  1881.000000  1897.900024  1682.720947   \n",
       " ...                 ...          ...          ...          ...          ...   \n",
       " 2023-12-22  3800.000000  3845.949951  3762.000000  3824.000000  3778.875244   \n",
       " 2023-12-26  3819.850098  3834.000000  3790.149902  3795.550049  3750.760986   \n",
       " 2023-12-27  3799.000000  3818.199951  3768.000000  3811.199951  3766.226074   \n",
       " 2023-12-28  3824.000000  3838.000000  3792.100098  3799.899902  3755.059326   \n",
       " 2023-12-29  3792.000000  3822.600098  3765.399902  3793.399902  3748.636230   \n",
       " \n",
       "              Volume  \n",
       " Date                 \n",
       " 2019-01-01  1094883  \n",
       " 2019-01-02  2100463  \n",
       " 2019-01-03  2611668  \n",
       " 2019-01-04  4280862  \n",
       " 2019-01-07  1856423  \n",
       " ...             ...  \n",
       " 2023-12-22  2413058  \n",
       " 2023-12-26  1285231  \n",
       " 2023-12-27  1293976  \n",
       " 2023-12-28  1682889  \n",
       " 2023-12-29  1574996  \n",
       " \n",
       " [1235 rows x 6 columns]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a38dc72-f8ee-4ff5-8d11-0e663d6ed678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bond data loaded and preprocessed.\n"
     ]
    }
   ],
   "source": [
    "# Shell 3: Load bond data and preprocess\n",
    "bond_data = pd.read_csv(\"/Users/raghavgarg/Downloads/bond.csv\")\n",
    "bond_data['Date'] = pd.to_datetime(bond_data['Date'], format='%d-%m-%Y')\n",
    "bond_data['Change %'] = bond_data['Change %'].str.rstrip('%').astype('float') / 100.0\n",
    "\n",
    "# Convert other columns to float as necessary\n",
    "bond_data.iloc[:, 1:5] = bond_data.iloc[:, 1:5].astype(float)\n",
    "\n",
    "print(\"Bond data loaded and preprocessed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b22b8f7-d022-47e9-a4a0-23aab1eb87b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Shell 4: Load and process stock data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#stock_data = pd.read_csv(\"stock_data.csv\")  # Replace with actual stock data file\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m stock_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(stock_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Example: Moving averages\u001b[39;00m\n\u001b[1;32m      6\u001b[0m stock_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMA_20\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m stock_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Date'"
     ]
    }
   ],
   "source": [
    "# Shell 4: Load and process stock data\n",
    "#stock_data = pd.read_csv(\"stock_data.csv\")  # Replace with actual stock data file\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "\n",
    "# Example: Moving averages\n",
    "stock_data['MA_20'] = stock_data['Close'].rolling(window=20).mean()\n",
    "stock_data['MA_50'] = stock_data['Close'].rolling(window=50).mean()\n",
    "\n",
    "# Preprocess sentiment data (assuming you have preprocessed sentiment features)\n",
    "sentiment_data = pd.read_csv(\"sentiment_data.csv\")  # Replace with actual sentiment data file\n",
    "sentiment_data['Date'] = pd.to_datetime(sentiment_data['Date'])\n",
    "\n",
    "# Merge all data (align on 'Date')\n",
    "merged_data = pd.merge(stock_data, sentiment_data, on='Date', how='inner')\n",
    "merged_data = pd.merge(merged_data, bond_data, on='Date', how='inner')\n",
    "print(\"Stock, sentiment, and bond data merged.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8f8ecae-be71-4423-a40e-4fc4748cb38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment data missing or malformed for RELIANCE.NS. Skipping sentiment merge.\n",
      "Sentiment data missing or malformed for SBIN.NS. Skipping sentiment merge.\n",
      "Sentiment data missing or malformed for HDFCBANK.NS. Skipping sentiment merge.\n",
      "Sentiment data missing or malformed for TCS.NS. Skipping sentiment merge.\n",
      "Stock and sentiment data prepared and merged for modeling.\n"
     ]
    }
   ],
   "source": [
    "# Shell 3: Prepare stock and sentiment data for merging and modeling\n",
    "merged_data = {}\n",
    "\n",
    "for symbol in stock_symbols:\n",
    "    stock_df = stock_data[symbol]\n",
    "    stock_df['Date'] = stock_df.index  # Extract date from the index\n",
    "    stock_df = stock_df[['Date', 'Close']].copy()\n",
    "\n",
    "    # Handle missing or malformed sentiment data\n",
    "    if symbol in sentiment_data and 'feed' in sentiment_data[symbol]:\n",
    "        sentiment_df = pd.DataFrame(sentiment_data[symbol].get('feed', []))\n",
    "\n",
    "        # Ensure the sentiment data contains a valid time field for 'Date'\n",
    "        if 'time_published' in sentiment_df.columns:\n",
    "            sentiment_df['Date'] = pd.to_datetime(sentiment_df['time_published'], errors='coerce')\n",
    "            sentiment_df.dropna(subset=['Date'], inplace=True)  # Remove rows with invalid dates\n",
    "            sentiment_df = sentiment_df.groupby('Date').mean().reset_index()  # Average sentiment for each day\n",
    "        else:\n",
    "            print(f\"No 'time_published' column found in sentiment data for {symbol}. Skipping sentiment merge.\")\n",
    "            sentiment_df = pd.DataFrame()  # Empty DataFrame if 'time_published' is missing\n",
    "    else:\n",
    "        print(f\"Sentiment data missing or malformed for {symbol}. Skipping sentiment merge.\")\n",
    "        sentiment_df = pd.DataFrame()  # Empty DataFrame if no valid sentiment data\n",
    "\n",
    "    # Merge stock and sentiment data on 'Date' if sentiment data is not empty\n",
    "    if not sentiment_df.empty:\n",
    "        merged_df = pd.merge(stock_df, sentiment_df, on='Date', how='inner')\n",
    "    else:\n",
    "        merged_df = stock_df  # If no sentiment data, use only stock data\n",
    "\n",
    "    merged_data[symbol] = merged_df  # Store the merged dataset in a dictionary\n",
    "    \n",
    "    # Save merged data (optional)\n",
    "    merged_df.to_csv(f'{symbol}_merged_data.csv', index=False)\n",
    "\n",
    "print(\"Stock and sentiment data prepared and merged for modeling.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c2794f4-09c4-4960-bbb6-fba963565887",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot insert Date, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z2/6hyggfgn2_scg_7jf9d391vr0000gn/T/ipykernel_23403/3634322096.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstock_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Copy to avoid modifying the original data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstock_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stock_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msymbol\u001b[0m  \u001b[0;31m# Add the stock symbol as a new column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Reset the index to make 'Date' a column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mstock_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Append this stock's data to the combined DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcombined_stock_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombined_stock_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstock_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[0m\n\u001b[1;32m   6468\u001b[0m                     level_values = algorithms.take(\n\u001b[1;32m   6469\u001b[0m                         \u001b[0mlevel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_na_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6470\u001b[0m                     \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6472\u001b[0;31m                 new_obj.insert(\n\u001b[0m\u001b[1;32m   6473\u001b[0m                     \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6474\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6475\u001b[0m                     \u001b[0mlevel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   5154\u001b[0m                 \u001b[0;34m\"'self.flags.allows_duplicate_labels' is False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5155\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5157\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5158\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34mcannot insert \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m, already exists\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5160\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loc must be int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5161\u001b[0m         \u001b[0;31m# convert non stdlib ints to satisfy typing checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot insert Date, already exists"
     ]
    }
   ],
   "source": [
    "# Shell 1: Combine stock data into a single DataFrame with the \"stock_name\" column\n",
    "combined_stock_data = pd.DataFrame()  # Empty DataFrame to store all stock data\n",
    "\n",
    "for symbol in stock_symbols:\n",
    "    # Add a 'stock_name' column to each stock's DataFrame\n",
    "    stock_df = stock_data[symbol].copy()  # Copy to avoid modifying the original data\n",
    "    stock_df['stock_name'] = symbol  # Add the stock symbol as a new column\n",
    "    \n",
    "    # Reset the index to make 'Date' a column\n",
    "    stock_df = stock_df.reset_index()\n",
    "\n",
    "    # Append this stock's data to the combined DataFrame\n",
    "    combined_stock_data = pd.concat([combined_stock_data, stock_df])\n",
    "\n",
    "# Save the combined stock data to a CSV file\n",
    "combined_stock_data.to_csv('combined_stock_data.csv', index=False)\n",
    "\n",
    "print(\"Combined stock data saved to 'combined_stock_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac36d0fc-1b50-4171-85c1-2e8bdd52cc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and custom RMSE metric defined.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched sentiment data for RELIANCE.NS\n",
      "Fetched sentiment data for SBIN.NS\n",
      "Fetched sentiment data for HDFCBANK.NS\n",
      "Fetched sentiment data for TCS.NS\n",
      "Stock prices and sentiment data fetched and saved.\n",
      "Bond data loaded and preprocessed.\n"
     ]
    }
   ],
   "source": [
    "# Shell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Custom metric to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "print(\"Libraries imported and custom RMSE metric defined.\")\n",
    "\n",
    "# Shell 2: Fetch stock price data and sentiment data\n",
    "stock_symbols = ['RELIANCE.NS', 'SBIN.NS', 'HDFCBANK.NS', 'TCS.NS']\n",
    "api_key = 'NPWUSQC1723OZ4YW'\n",
    "stock_data = {}\n",
    "sentiment_data = {}\n",
    "\n",
    "# Step 1: Fetch stock price data using Yahoo Finance\n",
    "for symbol in stock_symbols:\n",
    "    stock_data[symbol] = yf.download(symbol, start='2019-01-01', end='2024-01-01')\n",
    "    stock_data[symbol].to_csv(f'{symbol}_prices.csv')  # Optional saving as CSV\n",
    "\n",
    "# Step 2: Fetch sentiment data using Alpha Vantage API\n",
    "for symbol in stock_symbols:\n",
    "    url_sentiment = f'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={symbol}&apikey={api_key}'\n",
    "    response_sentiment = requests.get(url_sentiment)\n",
    "    \n",
    "    if response_sentiment.status_code == 200:\n",
    "        sentiment_data[symbol] = response_sentiment.json()\n",
    "        print(f\"Fetched sentiment data for {symbol}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch sentiment data for {symbol}: {response_sentiment.status_code}\")\n",
    "    \n",
    "    time.sleep(15)  # To handle API rate limits\n",
    "\n",
    "# Optionally save sentiment data\n",
    "for symbol, data in sentiment_data.items():\n",
    "    df_sentiment = pd.DataFrame(data.get('feed', []))  # Assuming sentiment data is under 'feed'\n",
    "    df_sentiment.to_csv(f'{symbol}_sentiment.csv', index=False)\n",
    "\n",
    "print(\"Stock prices and sentiment data fetched and saved.\")\n",
    "\n",
    "# Shell 3: Load bond data and preprocess\n",
    "bond_data = pd.read_csv(\"/Users/raghavgarg/Downloads/bond.csv\")\n",
    "bond_data['Date'] = pd.to_datetime(bond_data['Date'], format='%d-%m-%Y')\n",
    "bond_data['Change %'] = bond_data['Change %'].str.rstrip('%').astype('float') / 100.0\n",
    "\n",
    "# Convert other columns to float as necessary\n",
    "bond_data.iloc[:, 1:5] = bond_data.iloc[:, 1:5].astype(float)\n",
    "\n",
    "print(\"Bond data loaded and preprocessed.\")\n",
    "\n",
    "# Shell 4: Load and process stock data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "746e9284-0f03-4e65-87df-1605dabd2579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce1c18b-81f4-40f8-b6a8-9b68e6b59c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Change %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>7.196</td>\n",
       "      <td>7.207</td>\n",
       "      <td>7.207</td>\n",
       "      <td>7.192</td>\n",
       "      <td>0.0028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>7.176</td>\n",
       "      <td>7.225</td>\n",
       "      <td>7.225</td>\n",
       "      <td>7.172</td>\n",
       "      <td>-0.0043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>7.207</td>\n",
       "      <td>7.200</td>\n",
       "      <td>7.213</td>\n",
       "      <td>7.191</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>7.205</td>\n",
       "      <td>7.191</td>\n",
       "      <td>7.208</td>\n",
       "      <td>7.182</td>\n",
       "      <td>0.0031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-12-26</td>\n",
       "      <td>7.183</td>\n",
       "      <td>7.213</td>\n",
       "      <td>7.213</td>\n",
       "      <td>7.177</td>\n",
       "      <td>-0.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>7.508</td>\n",
       "      <td>7.467</td>\n",
       "      <td>7.519</td>\n",
       "      <td>7.439</td>\n",
       "      <td>0.0081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>7.448</td>\n",
       "      <td>7.407</td>\n",
       "      <td>7.475</td>\n",
       "      <td>7.406</td>\n",
       "      <td>0.0028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>7.427</td>\n",
       "      <td>7.397</td>\n",
       "      <td>7.433</td>\n",
       "      <td>7.381</td>\n",
       "      <td>0.0099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>7.354</td>\n",
       "      <td>7.409</td>\n",
       "      <td>7.415</td>\n",
       "      <td>7.350</td>\n",
       "      <td>-0.0086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>7.418</td>\n",
       "      <td>7.401</td>\n",
       "      <td>7.447</td>\n",
       "      <td>7.385</td>\n",
       "      <td>0.0065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1215 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  Price   Open   High    Low  Change %\n",
       "0    2024-01-01  7.196  7.207  7.207  7.192    0.0028\n",
       "1    2023-12-29  7.176  7.225  7.225  7.172   -0.0043\n",
       "2    2023-12-28  7.207  7.200  7.213  7.191    0.0003\n",
       "3    2023-12-27  7.205  7.191  7.208  7.182    0.0031\n",
       "4    2023-12-26  7.183  7.213  7.213  7.177   -0.0007\n",
       "...         ...    ...    ...    ...    ...       ...\n",
       "1210 2019-01-07  7.508  7.467  7.519  7.439    0.0081\n",
       "1211 2019-01-04  7.448  7.407  7.475  7.406    0.0028\n",
       "1212 2019-01-03  7.427  7.397  7.433  7.381    0.0099\n",
       "1213 2019-01-02  7.354  7.409  7.415  7.350   -0.0086\n",
       "1214 2019-01-01  7.418  7.401  7.447  7.385    0.0065\n",
       "\n",
       "[1215 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bond_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "673c386d-12b1-455b-8801-64efec7c241e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RELIANCE.NS':                    Open         High          Low        Close    Adj Close  \\\n",
       " Date                                                                          \n",
       " 2019-01-01  1028.852905  1030.727295  1015.000732  1024.966919  1000.963013   \n",
       " 2019-01-02  1019.023804  1030.453003  1006.680298  1011.617737   987.926453   \n",
       " 2019-01-03  1012.623474  1019.115234   996.714111   999.137085   975.738098   \n",
       " 2019-01-04  1003.388733  1009.834778   988.485107  1004.531616   981.006287   \n",
       " 2019-01-07  1012.166321  1022.635437  1006.680298  1010.109070   986.453064   \n",
       " ...                 ...          ...          ...          ...          ...   \n",
       " 2023-12-22  2559.600098  2580.899902  2547.649902  2565.050049  2556.373779   \n",
       " 2023-12-26  2568.000000  2591.949951  2562.699951  2578.050049  2569.329834   \n",
       " 2023-12-27  2582.000000  2599.899902  2573.100098  2586.850098  2578.100098   \n",
       " 2023-12-28  2589.800049  2612.000000  2586.850098  2605.550049  2596.736816   \n",
       " 2023-12-29  2611.100098  2614.000000  2579.300049  2584.949951  2576.206299   \n",
       " \n",
       "              Volume  \n",
       " Date                 \n",
       " 2019-01-01  4873335  \n",
       " 2019-01-02  7814409  \n",
       " 2019-01-03  8144143  \n",
       " 2019-01-04  9258272  \n",
       " 2019-01-07  6030145  \n",
       " ...             ...  \n",
       " 2023-12-22  8270892  \n",
       " 2023-12-26  3732832  \n",
       " 2023-12-27  4602078  \n",
       " 2023-12-28  6151318  \n",
       " 2023-12-29  5432292  \n",
       " \n",
       " [1235 rows x 6 columns],\n",
       " 'SBIN.NS':                   Open        High         Low       Close   Adj Close  \\\n",
       " Date                                                                     \n",
       " 2019-01-01  297.500000  300.700012  293.850006  299.600006  281.996826   \n",
       " 2019-01-02  299.100006  302.500000  293.100006  293.899994  276.631714   \n",
       " 2019-01-03  295.000000  295.549988  290.100006  291.100006  273.996277   \n",
       " 2019-01-04  292.100006  299.000000  291.500000  297.649994  280.161377   \n",
       " 2019-01-07  301.049988  301.500000  295.200012  296.299988  278.890686   \n",
       " ...                ...         ...         ...         ...         ...   \n",
       " 2023-12-22  644.750000  649.400024  635.150024  636.750000  626.248047   \n",
       " 2023-12-26  638.849976  641.299988  635.650024  638.049988  627.526550   \n",
       " 2023-12-27  640.750000  649.450012  639.000000  648.549988  637.853394   \n",
       " 2023-12-28  650.250000  653.299988  646.500000  651.400024  640.656433   \n",
       " 2023-12-29  645.500000  649.599976  639.549988  642.049988  631.460632   \n",
       " \n",
       "               Volume  \n",
       " Date                  \n",
       " 2019-01-01  11837127  \n",
       " 2019-01-02  25559853  \n",
       " 2019-01-03  17548347  \n",
       " 2019-01-04  19514041  \n",
       " 2019-01-07  14579399  \n",
       " ...              ...  \n",
       " 2023-12-22  14998068  \n",
       " 2023-12-26  10153089  \n",
       " 2023-12-27  14417646  \n",
       " 2023-12-28  16982092  \n",
       " 2023-12-29  13221898  \n",
       " \n",
       " [1235 rows x 6 columns],\n",
       " 'HDFCBANK.NS':                    Open         High          Low        Close    Adj Close  \\\n",
       " Date                                                                          \n",
       " 2019-01-01  1063.824951  1075.500000  1052.800049  1074.050049  1022.420776   \n",
       " 2019-01-02  1071.400024  1073.750000  1059.849976  1064.250000  1013.091858   \n",
       " 2019-01-03  1062.099976  1064.125000  1051.500000  1055.900024  1005.143188   \n",
       " 2019-01-04  1057.625000  1064.250000  1055.175049  1058.724976  1007.832336   \n",
       " 2019-01-07  1063.849976  1067.675049  1059.000000  1060.324951  1009.355347   \n",
       " ...                 ...          ...          ...          ...          ...   \n",
       " 2023-12-22  1683.599976  1685.900024  1667.099976  1670.849976  1648.341187   \n",
       " 2023-12-26  1673.250000  1685.949951  1668.550049  1682.449951  1659.784912   \n",
       " 2023-12-27  1681.500000  1706.500000  1678.599976  1703.300049  1680.354126   \n",
       " 2023-12-28  1709.300049  1721.400024  1702.000000  1705.250000  1682.277710   \n",
       " 2023-12-29  1697.000000  1714.900024  1696.000000  1709.250000  1686.223877   \n",
       " \n",
       "               Volume  \n",
       " Date                  \n",
       " 2019-01-01   3186720  \n",
       " 2019-01-02   4067116  \n",
       " 2019-01-03   6385832  \n",
       " 2019-01-04   3643560  \n",
       " 2019-01-07   2693506  \n",
       " ...              ...  \n",
       " 2023-12-22  24289425  \n",
       " 2023-12-26   9022928  \n",
       " 2023-12-27  13504539  \n",
       " 2023-12-28  22038235  \n",
       " 2023-12-29  12505713  \n",
       " \n",
       " [1235 rows x 6 columns],\n",
       " 'TCS.NS':                    Open         High          Low        Close    Adj Close  \\\n",
       " Date                                                                          \n",
       " 2019-01-01  1896.000000  1910.000000  1885.000000  1902.800049  1687.065796   \n",
       " 2019-01-02  1905.000000  1934.449951  1900.000000  1923.300049  1705.241699   \n",
       " 2019-01-03  1919.000000  1944.949951  1893.099976  1899.949951  1684.538818   \n",
       " 2019-01-04  1900.000000  1901.199951  1841.000000  1876.849976  1664.057617   \n",
       " 2019-01-07  1891.800049  1908.800049  1881.000000  1897.900024  1682.721069   \n",
       " ...                 ...          ...          ...          ...          ...   \n",
       " 2023-12-22  3800.000000  3845.949951  3762.000000  3824.000000  3778.875244   \n",
       " 2023-12-26  3819.850098  3834.000000  3790.149902  3795.550049  3750.760986   \n",
       " 2023-12-27  3799.000000  3818.199951  3768.000000  3811.199951  3766.226318   \n",
       " 2023-12-28  3824.000000  3838.000000  3792.100098  3799.899902  3755.059326   \n",
       " 2023-12-29  3792.000000  3822.600098  3765.399902  3793.399902  3748.635986   \n",
       " \n",
       "              Volume  \n",
       " Date                 \n",
       " 2019-01-01  1094883  \n",
       " 2019-01-02  2100463  \n",
       " 2019-01-03  2611668  \n",
       " 2019-01-04  4280862  \n",
       " 2019-01-07  1856423  \n",
       " ...             ...  \n",
       " 2023-12-22  2413058  \n",
       " 2023-12-26  1285231  \n",
       " 2023-12-27  1293976  \n",
       " 2023-12-28  1682889  \n",
       " 2023-12-29  1574996  \n",
       " \n",
       " [1235 rows x 6 columns]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7863f54c-0c51-4127-a0b5-66b4229c6e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/raghavgarg/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock, sentiment, and bond data merged for RELIANCE.NS.\n",
      "Empty DataFrame\n",
      "Columns: [Open_x, High_x, Low_x, Close, Adj Close, Volume, MA_20, MA_50, neg, neu, pos, compound, Date, Price, Open_y, High_y, Low_y, Change %]\n",
      "Index: []\n",
      "Stock, sentiment, and bond data merged for SBIN.NS.\n",
      "Empty DataFrame\n",
      "Columns: [Open_x, High_x, Low_x, Close, Adj Close, Volume, MA_20, MA_50, neg, neu, pos, compound, Date, Price, Open_y, High_y, Low_y, Change %]\n",
      "Index: []\n",
      "Stock, sentiment, and bond data merged for HDFCBANK.NS.\n",
      "Empty DataFrame\n",
      "Columns: [Open_x, High_x, Low_x, Close, Adj Close, Volume, MA_20, MA_50, neg, neu, pos, compound, Date, Price, Open_y, High_y, Low_y, Change %]\n",
      "Index: []\n",
      "Stock, sentiment, and bond data merged for TCS.NS.\n",
      "Empty DataFrame\n",
      "Columns: [Open_x, High_x, Low_x, Close, Adj Close, Volume, MA_20, MA_50, neg, neu, pos, compound, Date, Price, Open_y, High_y, Low_y, Change %]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 10)) while a minimum of 1 is required by MinMaxScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Scale data using MinMaxScaler\u001b[39;00m\n\u001b[1;32m     42\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[0;32m---> 43\u001b[0m scaled_data \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(merged_data[features])\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Split data into training and testing sets\u001b[39;00m\n\u001b[1;32m     46\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(scaled_data))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:450\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:490\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    487\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m    489\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 490\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    491\u001b[0m     X,\n\u001b[1;32m    492\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_pass,\n\u001b[1;32m    493\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_array_api\u001b[38;5;241m.\u001b[39msupported_float_dtypes(xp),\n\u001b[1;32m    494\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    495\u001b[0m )\n\u001b[1;32m    497\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    498\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1072\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1072\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1073\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1074\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1075\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1076\u001b[0m         )\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1079\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 10)) while a minimum of 1 is required by MinMaxScaler."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Shell 4: Load and process stock data\n",
    "for symbol, data in stock_data.items():\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    \n",
    "    # Example: Moving averages\n",
    "    data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "    data['MA_50'] = data['Close'].rolling(window=50).mean()\n",
    "    \n",
    "    # Extract the sentiment features\n",
    "    sentiment_features = []\n",
    "    for article in data['Volume']:\n",
    "        sentiment = sia.polarity_scores(str(article))\n",
    "        sentiment_features.append(sentiment)\n",
    "    \n",
    "    # Create a DataFrame with the sentiment features\n",
    "    df_sentiment = pd.DataFrame(sentiment_features)\n",
    "    \n",
    "    # Merge the sentiment features with the stock data\n",
    "    merged_data = pd.merge(data, df_sentiment, left_index=True, right_index=True)\n",
    "    \n",
    "    # Merge the merged data with the bond data\n",
    "    merged_data = pd.merge(merged_data, bond_data, left_index=True, right_index=True)\n",
    "    \n",
    "    print(f\"Stock, sentiment, and bond data merged for {symbol}.\")\n",
    "    \n",
    "    # Print the merged data\n",
    "    print(merged_data.head())\n",
    "    \n",
    "# Shell 5: Prepare data for LSTM model\n",
    "# Define features and target\n",
    "features = ['Open_x', 'High_x', 'Low_x', 'Volume', 'MA_20', 'MA_50', 'compound', 'pos', 'neg', 'neu']\n",
    "target = 'Close'\n",
    "\n",
    "# Scale data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(merged_data[features])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(0.8 * len(scaled_data))\n",
    "train_data, test_data = scaled_data[0:train_size], scaled_data[train_size:len(scaled_data)]\n",
    "\n",
    "# Split data into input (X) and output (y)\n",
    "X_train, y_train = train_data[:, 0:10], train_data[:, 10]\n",
    "X_test, y_test = test_data[:, 0:10], test_data[:, 10]\n",
    "\n",
    "# Reshape data for LSTM model\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "# Shell 6: Build and train LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=[rmse])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_rmse', patience=5, min_delta=0.001)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Shell 7: Evaluate model performance\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print(f'MSE: {mse[0]}')\n",
    "print(f'RMSE: {mse[1]}')\n",
    "\n",
    "# Shell 8: Plot model performance\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history.history['rmse'], label='Training RMSE')\n",
    "plt.plot(history.history['val_rmse'], label='Validation RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8969a-dbbe-4d3d-b57c-7592b6f335ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
